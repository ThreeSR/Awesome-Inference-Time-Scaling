# Awesome-Inference-Time-Scaling
**Paper List of Inference/Test Time Scaling/Computing**

## ğŸ“– Paper List (Listed in Time Order)

ğŸ”¹ [Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps](https://arxiv.org/abs/2501.09732)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.09732)
- ğŸ‘¤ **Authors:** Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, T. Jaakkola, Xuhui Jia, Saining Xie
- ğŸ—“ï¸ **Date:** 2025-01-16
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.

ğŸ”¹ [O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning](https://arxiv.org/abs/2501.06458)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.06458)
- ğŸ‘¤ **Authors:** Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang
- ğŸ—“ï¸ **Date:** 2025-01-11
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** Building upon our previous investigations of O1 replication (Part 1: Journey Learning [Qin et al., 2024] and Part 2: Distillation [Huang et al., 2024]), this work explores the potential of inference-time scaling in large language models (LLMs) for medical reasoning tasks, ranging from diagnostic decision-making to treatment planning. Through extensive experiments on medical benchmarks of varying complexity (MedQA, Medbullets, and JAMA Clinical Challenges), our investigation reveals several key insights: (1) Increasing inference time does lead to improved performance. With a modest training set of 500 samples, our model yields substantial performance improvements of 6%-11%. (2) Task complexity directly correlates with the required length of reasoning chains, confirming the necessity of extended thought processes for challenging problems. (3) The differential diagnoses generated by our model adhere to the principles of the hypothetico-deductive method, producing a list of potential conditions that may explain a patient's symptoms and systematically narrowing these possibilities by evaluating the evidence. These findings demonstrate the promising synergy between inference-time scaling and journey learning in advancing LLMs' real-world clinical reasoning capabilities.

ğŸ”¹ [A General Framework for Inference-time Scaling and Steering of Diffusion Models](https://arxiv.org/abs/2501.06848)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.06848)
- ğŸ‘¤ **Authors:** Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
- ğŸ—“ï¸ **Date:** 2025-01-12
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** Diffusion models produce impressive results in modalities ranging from images and video to protein design and text. However, generating samples with user-specified properties remains a challenge. Recent research proposes fine-tuning models to maximize rewards that capture desired properties, but these methods require expensive training and are prone to mode collapse. In this work, we propose Feynman Kac (FK) steering, an inference-time framework for steering diffusion models with reward functions. FK steering works by sampling a system of multiple interacting diffusion processes, called particles, and resampling particles at intermediate steps based on scores computed using functions called potentials. Potentials are defined using rewards for intermediate states and are selected such that a high value indicates that the particle will yield a high-reward sample. We explore various choices of potentials, intermediate rewards, and samplers. We evaluate FK steering on text-to-image and text diffusion models. For steering text-to-image models with a human preference reward, we find that FK steering a 0.8B parameter model outperforms a 2.6B parameter fine-tuned model on prompt fidelity, with faster sampling and no training. For steering text diffusion models with rewards for text quality and specific text attributes, we find that FK steering generates lower perplexity, more linguistically acceptable outputs and enables gradient-free control of attributes like toxicity. Our results demonstrate that inference-time scaling and steering of diffusion models, even with off-the-shelf rewards, can provide significant sample quality gains and controllability benefits. Code is available at https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .

ğŸ”¹ [Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](https://arxiv.org/abs/2407.21787)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2407.21787)
- ğŸ‘¤ **Authors:** Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher R'e, Azalia Mirhoseini
- ğŸ—“ï¸ **Date:** 2024-07-31
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.

ğŸ”¹ [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/abs/2406.16838)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2406.16838)
- ğŸ‘¤ **Authors:** S. Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui
- ğŸ—“ï¸ **Date:** 2024-06-24
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.

ğŸ”¹ [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2408.03314)
- ğŸ‘¤ **Authors:** Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
- ğŸ—“ï¸ **Date:** 2024-08-06
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a"compute-optimal"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.

ğŸ”¹ [Fast and Robust Online Inference with Stochastic Gradient Descent via Random Scaling](https://arxiv.org/abs/2106.03156)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2106.03156)
- ğŸ‘¤ **Authors:** S. Lee, Yuan Liao, M. Seo, Youngki Shin
- ğŸ—“ï¸ **Date:** 2021-06-06
- ğŸ“‘ **Publisher:** AAAI Conference on Artificial Intelligence
- ğŸ“ **Abstract:** We develop a new method of online inference for a vector of parameters estimated by the Polyak-Ruppert averaging procedure of stochastic gradient descent (SGD) algorithms. We leverage insights from time series regression in econometrics and construct asymptotically pivotal statistics via random scaling. Our approach is fully operational with online data and is rigorously underpinned by a functional central limit theorem. Our proposed inference method has a couple of key advantages over the existing methods. First, the test statistic is computed in an online fashion with only SGD iterates and the critical values can be obtained without any resampling methods, thereby allowing for efficient implementation suitable for massive online data. Second, there is no need to estimate the asymptotic variance and our inference method is shown to be robust to changes in the tuning parameters for SGD algorithms in simulation experiments with synthetic data.

ğŸ”¹ [Scaling up GANs for Text-to-Image Synthesis](https://arxiv.org/abs/2303.05511)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2303.05511)
- ğŸ‘¤ **Authors:** Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park
- ğŸ—“ï¸ **Date:** 2023-03-09
- ğŸ“‘ **Publisher:** Computer Vision and Pattern Recognition
- ğŸ“ **Abstract:** The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL.E 2, autoregressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that naÃ¯vely increasing the capacity of the StyleGan architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.

ğŸ”¹ [Scaling Vision with Sparse Mixture of Experts](https://arxiv.org/abs/2106.05974)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2106.05974)
- ğŸ‘¤ **Authors:** C. Riquelme, J. Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, AndrÃ© Susano Pinto, Daniel Keysers, N. Houlsby
- ğŸ—“ï¸ **Date:** 2021-06-10
- ğŸ“‘ **Publisher:** Neural Information Processing Systems
- ğŸ“ **Abstract:** Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are"dense", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.

ğŸ”¹ [Flow Matching for Scalable Simulation-Based Inference](https://arxiv.org/abs/2305.17161)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2305.17161)
- ğŸ‘¤ **Authors:** Maximilian Dax, J. Wildberger, Simon Buchholz, Stephen R. Green, J. Macke, B. Scholkopf
- ğŸ—“ï¸ **Date:** 2023-05-26
- ğŸ“‘ **Publisher:** Neural Information Processing Systems
- ğŸ“ **Abstract:** Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.

ğŸ”¹ [Scaling transformer neural networks for skillful and reliable medium-range weather forecasting](https://arxiv.org/abs/2312.03876)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2312.03876)
- ğŸ‘¤ **Authors:** Tung Nguyen, Rohan Shah, Hritik Bansal, T. Arcomano, Sandeep Madireddy, R. Maulik, V. Kotamarthi, Ian Foster, Aditya Grover
- ğŸ—“ï¸ **Date:** 2023-12-06
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** Weather forecasting is a fundamental problem for anticipating and mitigating the impacts of climate change. Recently, data-driven approaches for weather forecasting based on deep learning have shown great promise, achieving accuracies that are competitive with operational systems. However, those methods often employ complex, customized architectures without sufficient ablation analysis, making it difficult to understand what truly contributes to their success. Here we introduce Stormer, a simple transformer model that achieves state-of-the-art performance on weather forecasting with minimal changes to the standard transformer backbone. We identify the key components of Stormer through careful empirical analyses, including weather-specific embedding, randomized dynamics forecast, and pressure-weighted loss. At the core of Stormer is a randomized forecasting objective that trains the model to forecast the weather dynamics over varying time intervals. During inference, this allows us to produce multiple forecasts for a target lead time and combine them to obtain better forecast accuracy. On WeatherBench 2, Stormer performs competitively at short to medium-range forecasts and outperforms current methods beyond 7 days, while requiring orders-of-magnitude less training data and compute. Additionally, we demonstrate Stormer's favorable scaling properties, showing consistent improvements in forecast accuracy with increases in model size and training tokens. Code and checkpoints are available at https://github.com/tung-nd/stormer.

ğŸ”¹ [A general linear-time inference method for Gaussian Processes on one dimension](https://arxiv.org/abs/2003.05554)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2003.05554)
- ğŸ‘¤ **Authors:** Jackson Loper, D. Blei, J. Cunningham, L. Paninski
- ğŸ—“ï¸ **Date:** 2020-03-11
- ğŸ“‘ **Publisher:** Journal of machine learning research
- ğŸ“ **Abstract:** Gaussian Processes (GPs) provide powerful probabilistic frameworks for interpolation, forecasting, and smoothing, but have been hampered by computational scaling issues. Here we investigate data sampled on one dimension (e.g., a scalar or vector time series sampled at arbitrarily-spaced intervals), for which state-space models are popular due to their linearly-scaling computational costs. It has long been conjectured that state-space models are general, able to approximate any one-dimensional GP. We provide the first general proof of this conjecture, showing that any stationary GP on one dimension with vector-valued observations governed by a Lebesgue-integrable continuous kernel can be approximated to any desired precision using a specifically-chosen state-space model: the Latent Exponentially Generated (LEG) family. This new family offers several advantages compared to the general state-space model: it is always stable (no unbounded growth), the covariance can be computed in closed form, and its parameter space is unconstrained (allowing straightforward estimation via gradient descent). The theorem's proof also draws connections to Spectral Mixture Kernels, providing insight about this popular family of kernels. We develop parallelized algorithms for performing inference and learning in the LEG model, test the algorithm on real and synthetic data, and demonstrate scaling to datasets with billions of samples.

ğŸ”¹ [INFless: a native serverless system for low-latency, high-throughput inference](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Yanan Yang, Laiping Zhao, Yiming Li, Huanyu Zhang, Jie Li, Mingyang Zhao, Xingzhen Chen, Keqiu Li
- ğŸ—“ï¸ **Date:** 2022-02-28
- ğŸ“‘ **Publisher:** International Conference on Architectural Support for Programming Languages and Operating Systems
- ğŸ“ **Abstract:** Modern websites increasingly rely on machine learning (ML) to improve their business efficiency. Developing and maintaining ML services incurs high costs for developers. Although serverless systems are a promising solution to reduce costs, we find that the current general purpose serverless systems cannot meet the low latency, high throughput demands of ML services. While simply â€patchingâ€ general serverless systems does not resolve the problem completely, we propose that such a system should natively combine the features of inference with a serverless paradigm. We present INFless, the first ML domain-specific serverless platform. It provides a unified, heterogeneous resource abstraction between CPU and accelerators, and achieves high throughput using built-in batching and non-uniform scaling mechanisms. It also supports low latency through coordinated management of batch queuing time, execution time and coldstart rate. We evaluate INFless using both a local cluster testbed and a large-scale simulation. Experimental results show that INFless outperforms state-of-the-art systems by 2Ã—-5Ã— on system throughput, meeting the latency goals of ML services.

ğŸ”¹ [Accelerating Distributed MoE Training and Inference with Lina](https://arxiv.org/abs/2210.17223)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2210.17223)
- ğŸ‘¤ **Authors:** Jiamin Li, Yimin Jiang, Yibo Zhu, Cong Wang, Hong-Yu Xu
- ğŸ—“ï¸ **Date:** 2022-10-31
- ğŸ“‘ **Publisher:** USENIX Annual Technical Conference
- ğŸ“ **Abstract:** Scaling model parameters improves model quality at the price of high computation overhead. Sparsely activated models, usually in the form of Mixture of Experts (MoE) architecture, have sub-linear scaling of computation cost with model size, thus providing opportunities to train and serve a larger model at lower cost than their dense counterparts. However, distributed MoE training and inference is inefficient, mainly due to the interleaved all-to-all communication during model computation. This paper makes two main contributions. First, we systematically analyze all-to-all overhead in distributed MoE and present the main causes for it to be the bottleneck in training and inference, respectively. Second, we design and build Lina to address the all-to-all bottleneck head-on. Lina opportunistically prioritizes all-to-all over the concurrent allreduce whenever feasible using tensor partitioning, so all-to-all and training step time is improved. Lina further exploits the inherent pattern of expert selection to dynamically schedule resources during inference, so that the transfer size and bandwidth of all-to-all across devices are balanced amid the highly skewed expert popularity in practice. Experiments on an A100 GPU testbed show that Lina reduces the training step time by up to 1.73x and reduces the 95%ile inference time by an average of 1.63x over the state-of-the-art systems.

ğŸ”¹ [Inference for heavy tailed stationary time series based on sliding blocks](https://arxiv.org/abs/1706.01968)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1706.01968)
- ğŸ‘¤ **Authors:** Axel Bucher, J. Segers
- ğŸ—“ï¸ **Date:** 2017-06-06
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** The block maxima method in extreme value theory consists of fitting an extreme value distribution to a sample of block maxima extracted from a time series. Traditionally, the maxima are taken over disjoint blocks of observations. Alternatively, the blocks can be chosen to slide through the observation period, yielding a larger number of overlapping blocks. Inference based on sliding blocks is found to be more efficient than inference based on disjoint blocks. The asymptotic variance of the maximum likelihood estimator of the Fr\'{e}chet shape parameter is reduced by more than 18%. Interestingly, the amount of the efficiency gain is the same whatever the serial dependence of the underlying time series: as for disjoint blocks, the asymptotic distribution depends on the serial dependence only through the sequence of scaling constants. The findings are illustrated by simulation experiments and are applied to the estimation of high return levels of the daily log-returns of the Standard & Poor's 500 stock market index.

ğŸ”¹ [A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference](https://arxiv.org/abs/2212.12393)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2212.12393)
- ğŸ‘¤ **Authors:** Emile van Krieken, Thiviyan Thanapalasingam, J. Tomczak, F. V. Harmelen, A. T. Teije
- ğŸ—“ï¸ **Date:** 2022-12-23
- ğŸ“‘ **Publisher:** Neural Information Processing Systems
- ğŸ“ **Abstract:** We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in performance.

ğŸ”¹ [MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan
- ğŸ—“ï¸ **Date:** None
- ğŸ“‘ **Publisher:** USENIX Annual Technical Conference
- ğŸ“ **Abstract:** The advances of Machine Learning (ML) have sparked a growing demand of ML-as-a-Service: developers train ML models and publish them in the cloud as online services to provide low-latency inference at scale. The key challenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizing the serving cost. In this paper, we tackle the dual challenge of SLO compliance and cost effectiveness with MArk (Model Ark), a general-purpose inference serving system built in Amazon Web Services (AWS). MArk employs three design choices tailor-made for inference workload. First, MArk dy-namically batches requests and opportunistically serves them using expensive hardware accelerators (e.g., GPU) for improved performance-cost ratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow or too expensive for inference serving, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature of inference serving, MArk exploits the ï¬‚exible, yet costly serverless instances to cover the occasional load spikes that are hard to predict. We evaluated the performance of MArk using several state-of-the-art ML models trained in popular frameworks including TensorFlow, MXNet, and Keras. Compared with the premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to 7 . 8 Ã— while achieving even better latency performance.

ğŸ”¹ [AutoScale: Energy Efficiency Optimization for Stochastic Edge Inference Using Reinforcement Learning](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Young Geun Kim, Carole-Jean Wu
- ğŸ—“ï¸ **Date:** 2020-10-01
- ğŸ“‘ **Publisher:** Micro
- ğŸ“ **Abstract:** Deep learning inference is increasingly run at the edge. As the programming and system stack support becomes mature, it enables acceleration opportunities in a mobile system, where the system performance envelope is scaled up with a plethora of programmable co-processors. Thus, intelligent services designed for mobile users can choose between running inference on the CPU or any of the co-processors in the mobile system, and exploiting connected systems such as the cloud or a nearby, locally connected mobile system. By doing so, these services can scale out the performance and increase the energy efficiency of edge mobile systems. This gives rise to a new challengeâ€”deciding when inference should run where. Such execution scaling decision becomes more complicated with the stochastic nature of mobile-cloud execution environment, where signal strength variation in the wireless networks and resource interference can affect real-time inference performance and system energy efficiency. To enable energy efficient deep learning inference at the edge, this paper proposes AutoScale, an adaptive and lightweight execution scaling engine built on the custom-designed reinforcement learning algorithm. It continuously learns and selects the most energy efficient inference execution target by considering characteristics of neural networks and available systems in the collaborative cloud-edge execution environment while adapting to stochastic runtime variance. Real system implementation and evaluation, considering realistic execution scenarios, demonstrate an average of 9.8x and 1.6x energy efficiency improvement over the baseline mobile CPU and cloud offloading, respectively, while meeting the real-time performance and accuracy requirements.

ğŸ”¹ [Bayesian time series models and scalable inference](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Matthew J. Johnson
- ğŸ—“ï¸ **Date:** None
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** With large and growing datasets and complex models, there is an increasing need for scalable Bayesian inference. We describe two lines of work to address this need. In the first part, we develop new algorithms for inference in hierarchical Bayesian time series models based on the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and their Bayesian nonparametric extensions. The HMM is ubiquitous in Bayesian time series models, and it and its Bayesian nonparametric extension, the hierarchical Dirichlet process hidden Markov model (HDP-HMM), have been applied in many settings. HSMMs and HDP-HSMMs extend these dynamical models to provide state-specific duration modeling, but at the cost of increased computational complexity for inference, limiting their general applicability. A challenge with all such models is scaling inference to large datasets. We address these challenges in several ways. First, we develop classes of duration models for which HSMM message passing complexity scales only linearly in the observation sequence length. Second, we apply the stochastic variational inference (SVI) framework to develop scalable inference for the HMM, HSMM, and their nonparametric extensions. Third, we build on these ideas to define a new Bayesian nonparametric model that can capture dynamics at multiple timescales while still allowing efficient and scalable inference. In the second part of this thesis, we develop a theoretical framework to analyze a special case of a highly parallelizable sampling strategy we refer to as Hogwild Gibbs sampling. Thorough empirical work has shown that Hogwild Gibbs sampling works very well for inference in large latent Dirichlet allocation models (LDA), but there is little theory to understand when it may be effective in general. By studying Hogwild Gibbs applied to sampling from Gaussian distributions we develop analytical results as well as a deeper understanding of its behavior, including its convergence and correctness in some regimes. Thesis Supervisor: Alan S. Willsky Professor of Electrical Engineering and Computer Science

ğŸ”¹ [A Self-Adaptive Online Brainâ€“Machine Interface of a Humanoid Robot Through a General Type-2 Fuzzy Inference System](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Javier Andreu-Perez, Fan Cao, H. Hagras, Guang-Zhong Yang
- ğŸ—“ï¸ **Date:** 2018-02-01
- ğŸ“‘ **Publisher:** IEEE transactions on fuzzy systems
- ğŸ“ **Abstract:** This paper presents a self-adaptive autonomous online learning through a general type-2 fuzzy system (GT2 FS) for the motor imagery (MI) decoding of a brain-machine interface (BMI) and navigation of a bipedal humanoid robot in a real experiment, using electroencephalography (EEG) brain recordings only. GT2 FSs are applied to BMI for the first time in this study. We also account for several constraints commonly associated with BMI in real practice: 1) the maximum number of EEG channels is limited and fixed; 2) no possibility of performing repeated user training sessions; and 3) desirable use of unsupervised and low-complexity feature extraction methods. The novel online learning method presented in this paper consists of a self-adaptive GT2 FS that can autonomously self-adapt both its parameters and structure via creation, fusion, and scaling of the fuzzy system rules in an online BMI experiment with a real robot. The structure identification is based on an online GT2 Gathâ€“Geva algorithm where every MI decoding class can be represented by multiple fuzzy rules (models), which are learnt in a continous (trial-by-trial) non-iterative basis. The effectiveness of the proposed method is demonstrated in a detailed BMI experiment, in which 15 untrained users were able to accurately interface with a humanoid robot, in a single session, using signals from six EEG electrodes only.

ğŸ”¹ [Metric Gaussian Variational Inference](https://arxiv.org/abs/1901.11033)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1901.11033)
- ğŸ‘¤ **Authors:** Jakob KnollmÃ¼ller, T. Ensslin
- ğŸ—“ï¸ **Date:** 2019-01-30
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** Solving Bayesian inference problems approximately with variational approaches can provide fast and accurate results. Capturing correlation within the approximation requires an explicit parametrization. This intrinsically limits this approach to either moderately dimensional problems, or requiring the strongly simplifying mean-field approach. We propose Metric Gaussian Variational Inference (MGVI) as a method that goes beyond mean-field. Here correlations between all model parameters are taken into account, while still scaling linearly in computational time and memory. With this method we achieve higher accuracy and in many cases a significant speedup compared to traditional methods. MGVI is an iterative method that performs a series of Gaussian approximations to the posterior. We alternate between approximating the covariance with the inverse Fisher information metric evaluated at an intermediate mean estimate and optimizing the KL-divergence for the given covariance with respect to the mean. This procedure is iterated until the uncertainty estimate is self-consistent with the mean parameter. We achieve linear scaling by avoiding to store the covariance explicitly at any time. Instead we draw samples from the approximating distribution relying on an implicit representation and numerical schemes to approximately solve linear equations. Those samples are used to approximate the KL-divergence and its gradient. The usage of natural gradient descent allows for rapid convergence. Formulating the Bayesian model in standardized coordinates makes MGVI applicable to any inference problem with continuous parameters. We demonstrate the high accuracy of MGVI by comparing it to HMC and its fast convergence relative to other established methods in several examples. We investigate real-data applications, as well as synthetic examples of varying size and complexity and up to a million model parameters.

ğŸ”¹ [Scaling Video Analytics Systems to Large Camera Deployments](https://arxiv.org/abs/1809.02318)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1809.02318)
- ğŸ‘¤ **Authors:** Samvit Jain, Ganesh Ananthanarayanan, Junchen Jiang, Yuanchao Shu, Joseph E. Gonzalez
- ğŸ—“ï¸ **Date:** 2018-09-07
- ğŸ“‘ **Publisher:** Workshop on Mobile Computing Systems and Applications
- ğŸ“ **Abstract:** Driven by advances in computer vision and the falling costs of camera hardware, organizations are deploying video cameras en masse for the spatial monitoring of their physical premises. Scaling video analytics to massive camera deployments, however, presents a new and mounting challenge, as compute cost grows proportionally to the number of camera feeds. This paper is driven by a simple question: can we scale video analytics in such a way that cost grows sublinearly, or even remains constant, as we deploy more cameras, while inference accuracy remains stable, or even improves. We believe the answer is yes. Our key observation is that video feeds from wide-area camera deployments demonstrate significant content correlations (e.g. to other geographically proximate feeds), both in space and over time. These spatio-temporal correlations can be harnessed to dramatically reduce the size of the inference search space, decreasing both workload and false positive rates in multi-camera video analytics. By discussing use-cases and technical challenges, we propose a roadmap for scaling video analytics to large camera networks, and outline a plan for its realization.

ğŸ”¹ [Scalable Exact Inference in Multi-Output Gaussian Processes](https://arxiv.org/abs/1911.06287)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1911.06287)
- ğŸ‘¤ **Authors:** W. Bruinsma, E. Perim, Will Tebbutt, J. Hosking, A. Solin, Richard E. Turner
- ğŸ—“ï¸ **Date:** 2019-11-14
- ğŸ“‘ **Publisher:** International Conference on Machine Learning
- ğŸ“ **Abstract:** Multi-output Gaussian processes (MOGPs) leverage the flexibility and interpretability of GPs while capturing structure across outputs, which is desirable, for example, in spatio-temporal modelling. The key problem with MOGPs is their computational scaling $O(n^3 p^3)$, which is cubic in the number of both inputs $n$ (e.g., time points or locations) and outputs $p$. For this reason, a popular class of MOGPs assumes that the data live around a low-dimensional linear subspace, reducing the complexity to $O(n^3 m^3)$. However, this cost is still cubic in the dimensionality of the subspace $m$, which is still prohibitively expensive for many applications. We propose the use of a sufficient statistic of the data to accelerate inference and learning in MOGPs with orthogonal bases. The method achieves linear scaling in $m$ in practice, allowing these models to scale to large $m$ without sacrificing significant expressivity or requiring approximation. This advance opens up a wide range of real-world tasks and can be combined with existing GP approximations in a plug-and-play way. We demonstrate the efficacy of the method on various synthetic and real-world data sets.

ğŸ”¹ [FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours](https://arxiv.org/abs/2203.00854)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2203.00854)
- ğŸ‘¤ **Authors:** Shenggan Cheng, R. Wu, Zhongming Yu, Bin-Rui Li, Xiwen Zhang, Jian Peng, Yang You
- ğŸ—“ï¸ **Date:** 2022-03-02
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** Protein structure prediction helps to understand gene translation and protein function, which is of growing interest and importance in structural biology. The AlphaFold model, which used transformer architecture to achieve atomic-level accuracy in protein structure prediction, was a significant breakthrough. However, training and inference of the AlphaFold model are challenging due to its high computation and memory cost. In this work, we present FastFold, an efficient implementation of AlphaFold for both training and inference. We propose Dynamic Axial Parallelism and Duality Async Operations to improve the scaling efficiency of model parallelism. Besides, AutoChunk is proposed to reduce memory cost by over 80% during inference by automatically determining the chunk strategy. Experimental results show that FastFold reduces overall training time from 11 days to 67 hours and achieves 7.5X - 9.5X speedup for long-sequence inference. Furthermore, we scale FastFold to 512 GPUs and achieve an aggregate throughput of 6.02 PetaFLOP/s with 90.1% parallel efficiency.

ğŸ”¹ [Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Jie Huang, Peng Fei Zhu, Mingrui Geng, Jie Ran, Xingguang Zhou, Chen Xing, Pengfei Wan, Xiangyang Ji
- ğŸ—“ï¸ **Date:** 2018-09-08
- ğŸ“‘ **Publisher:** ECCV Workshops
- ğŸ“ **Abstract:** None

ğŸ”¹ [GrandPrix: scaling up the Bayesian GPLVM for single-cell data](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Sumon Ahmed, M. Rattray, A. Boukouvalas
- ğŸ—“ï¸ **Date:** 2017-12-03
- ğŸ“‘ **Publisher:** bioRxiv
- ğŸ“ **Abstract:** Motivation The Gaussian Process LatentVariable Model (GPLVM) is a popular approach for dimensionality reduction of single-cell data and has been used for pseudotime estimation with capture time information. However current implementations are computationally intensive and will not scale up to modern droplet-based single-cell datasets which routinely profile many tens of thousands of cells. Results We provide an efficient implementation which allows scaling up this approach to modern single-cell datasets. We also generalize the application of pseudotime inference to cases where there are other sources of variation, such as branching dynamics. We applied our method on microarray, nCounter, RNA-seq, qPCR and droplet-based datasets from different organisms. The model converges an order of magnitude faster compared to existing methods whilst achieving similar levels of estimation accuracy. Further, we demonstrate the flexibility of our approach by extending the model to higher-dimensional latent spaces that can be used to simultaneously infer pseudotime and other structure such as branching. Thus, the model has the capability of producing meaningful biological insights about cell ordering as well as cell fate regulation. Availability Software available at github.com/ManchesterBioinference/GrandPrix.

ğŸ”¹ [Scaling up Dynamic Topic Models](https://arxiv.org/abs/1602.06049)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1602.06049)
- ğŸ‘¤ **Authors:** Arnab Bhadury, Jianfei Chen, Jun Zhu, Shixia Liu
- ğŸ—“ï¸ **Date:** 2016-02-19
- ğŸ“‘ **Publisher:** The Web Conference
- ğŸ“ **Abstract:** Dynamic topic models (DTMs) are very effective in discovering topics and capturing their evolution trends in time series data. To do posterior inference of DTMs, existing methods are all batch algorithms that scan the full dataset before each update of the model and make inexact variational approximations with mean-field assumptions. Due to a lack of a more scalable inference algorithm, despite the usefulness, DTMs have not captured large topic dynamics. This paper fills this research void, and presents a fast and parallelizable inference algorithm using Gibbs Sampling with Stochastic Gradient Langevin Dynamics that does not make any unwarranted assumptions. We also present a Metropolis-Hastings based $O(1)$ sampler for topic assignments for each word token. In a distributed environment, our algorithm requires very little communication between workers during sampling (almost embarrassingly parallel) and scales up to large-scale applications. We are able to learn the largest Dynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topics from 2.6 million documents in less than half an hour, and our empirical results show that our algorithm is not only orders of magnitude faster than the baselines but also achieves lower perplexity.

ğŸ”¹ [Scaling up Data Augmentation MCMC via Calibration](https://arxiv.org/abs/1703.03123)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1703.03123)
- ğŸ‘¤ **Authors:** L. Duan, J. Johndrow, D. Dunson
- ğŸ—“ï¸ **Date:** 2017-03-09
- ğŸ“‘ **Publisher:** Journal of machine learning research
- ğŸ“ **Abstract:** There has been considerable interest in making Bayesian inference more scalable. In big data settings, most literature focuses on reducing the computing time per iteration, with less focused on reducing the number of iterations needed in Markov chain Monte Carlo (MCMC). This article focuses on data augmentation MCMC (DA-MCMC), a widely used technique. DA-MCMC samples tend to become highly autocorrelated in large data samples, due to a miscalibration problem in which conditional posterior distributions given augmented data are too concentrated. This makes it necessary to collect very long MCMC paths to obtain acceptably low MC error. To combat this inefficiency, we propose a family of calibrated data augmentation algorithms, which appropriately adjust the variance of conditional posterior distributions. A Metropolis-Hastings step is used to eliminate bias in the stationary distribution of the resulting sampler. Compared to existing alternatives, this approach can dramatically reduce MC error by reducing autocorrelation and increasing the effective number of DA-MCMC samples per computing time. The approach is simple and applicable to a broad variety of existing data augmentation algorithms, and we focus on three popular models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications.

ğŸ”¹ [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/abs/2404.02905)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2404.02905)
- ğŸ‘¤ **Authors:** Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang
- ğŸ—“ï¸ **Date:** 2024-04-03
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine"next-scale prediction"or"next-resolution prediction", diverging from the standard raster-scan"next-token prediction". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.

ğŸ”¹ [Simultaneous nonparametric inference of time series](https://arxiv.org/abs/1010.4182)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1010.4182)
- ğŸ‘¤ **Authors:** Weidong Liu, W. Wu
- ğŸ—“ï¸ **Date:** 2010-10-20
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** We consider kernel estimation of marginal densities and regression functions of stationary processes. It is shown that for a wide class of time series, with proper centering and scaling, the maximum deviations of kernel density and regression estimates are asymptotically Gumbel. Our results substantially generalize earlier ones which were obtained under independence or beta mixing assumptions. The asymptotic results can be applied to assess patterns of marginal densities or regression functions via the construction of simultaneous confidence bands for which one can perform goodness-of-fit tests. As an application, we construct simultaneous confidence bands for drift and volatility functions in a dynamic short-term rate model for the U.S. Treasury yield curve rates data.

ğŸ”¹ [RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness](https://arxiv.org/abs/2405.17220)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2405.17220)
- ğŸ‘¤ **Authors:** Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Dawn Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun
- ğŸ—“ï¸ **Date:** 2024-05-27
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** Traditional feedback learning for hallucination reduction relies on labor-intensive manual labeling or expensive proprietary models. This leaves the community without foundational knowledge about how to build high-quality feedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally explores open-source MLLMs from two perspectives, including high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling. Extensive experiments on six benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models at both preference learning and inference time. RLAIF-V 7B reduces object hallucination by 80.7\% and overall hallucination by 33.7\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of open-source MLLMs, where the model can learn from feedback of itself to achieve super GPT-4V trustworthiness.

ğŸ”¹ [Accurate Uncertainties for Deep Learning Using Calibrated Regression](https://arxiv.org/abs/1807.00263)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1807.00263)
- ğŸ‘¤ **Authors:** Volodymyr Kuleshov, Nathan Fenner, Stefano Ermon
- ğŸ—“ï¸ **Date:** 2018-07-01
- ğŸ“‘ **Publisher:** International Conference on Machine Learning
- ğŸ“ **Abstract:** Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.

ğŸ”¹ [A statistical test for the time constancy of scaling exponents](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** D. Veitch, P. Abry
- ğŸ—“ï¸ **Date:** 2001-10-01
- ğŸ“‘ **Publisher:** IEEE Transactions on Signal Processing
- ğŸ“ **Abstract:** A statistical test is described for determining if scaling exponents vary over time. It is applicable to diverse scaling phenomena including long-range dependence and exactly self-similar processes in a uniform framework without the need for prior knowledge of the type in question. It is based on the special properties of wavelet-based estimates of the scaling exponent, strongly motivating an idealized inference problem: the equality or otherwise of means of independent Gaussian variables with known variances. A uniformly most powerful invariant test exists for this problem and is described. A separate uniformly most powerful invariant test is also given for when the scaling exponent undergoes a level change. The power functions of both tests are given explicitly and compared. Using simulation, the effect, in practice, of deviations from the idealizations made of the statistical properties of the wavelet detail coefficients are analyzed and found to be small. The tests inherit the significant robustness and computational advantages of the underlying wavelet-based estimator. A detailed methodology is given, describing its use in practical situations. The use and benefits of the test are illustrated on the Bellcore Ethernet data sets.

ğŸ”¹ [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2109.10862)
- ğŸ‘¤ **Authors:** Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nissan Stiennon, Ryan Lowe, J. Leike, P. Christiano
- ğŸ—“ï¸ **Date:** 2021-09-22
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.

ğŸ”¹ [Time - Space Trade-Offs in Scaling up RDF Schema Reasoning](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** H. Stuckenschmidt, J. Broekstra
- ğŸ—“ï¸ **Date:** 2005-11-20
- ğŸ“‘ **Publisher:** WISE Workshops
- ğŸ“ **Abstract:** None

ğŸ”¹ [F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models](https://arxiv.org/abs/2209.15639)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2209.15639)
- ğŸ‘¤ **Authors:** Weicheng Kuo, Yin Cui, Xiuye Gu, A. Piergiovanni, A. Angelova
- ğŸ—“ï¸ **Date:** 2022-09-30
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** We present F-VLM, a simple open-vocabulary object detection method built upon Frozen Vision and Language Models. F-VLM simplifies the current multi-stage training pipeline by eliminating the need for knowledge distillation or detection-tailored pretraining. Surprisingly, we observe that a frozen VLM: 1) retains the locality-sensitive features necessary for detection, and 2) is a strong region classifier. We finetune only the detector head and combine the detector and VLM outputs for each region at inference time. F-VLM shows compelling scaling behavior and achieves +6.5 mask AP improvement over the previous state of the art on novel categories of LVIS open-vocabulary detection benchmark. In addition, we demonstrate very competitive results on COCO open-vocabulary detection benchmark and cross-dataset transfer detection, in addition to significant training speed-up and compute savings. Code will be released at the https://sites.google.com/view/f-vlm/home

ğŸ”¹ [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.10440)
- ğŸ‘¤ **Authors:** Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan
- ğŸ—“ï¸ **Date:** 2024-11-15
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-CoT-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-CoT not only outperforms its base model by 7.4% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.

ğŸ”¹ [Self-conditioned Embedding Diffusion for Text Generation](https://arxiv.org/abs/2211.04236)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2211.04236)
- ğŸ‘¤ **Authors:** Robin Strudel, Corentin Tallec, Florent Altch'e, Yilun Du, Yaroslav Ganin, A. Mensch, Will Grathwohl, Nikolay Savinov, S. Dieleman, L. Sifre, RÃ©mi Leblond
- ğŸ—“ï¸ **Date:** 2022-11-08
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** Can continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditioned Embedding Diffusion, a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. Through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models - while being in theory more efficient on accelerator hardware at inference time. Our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion.

ğŸ”¹ [Efficient Algorithms for Discrete Wavelet Transform: With Applications to Denoising and Fuzzy Inference Systems](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** K. K. Shukla, A. Tiwari
- ğŸ—“ï¸ **Date:** 2013-01-24
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** Due to its inherent time-scale locality characteristics, the discrete wavelet transform (DWT) has received considerable attention in signal/image processing. Wavelet transforms have excellent energy compaction characteristics and can provide perfect reconstruction. The shifting (translation) and scaling (dilation) are unique to wavelets. Orthogonality of wavelets with respect to dilations leads to multigrid representation. As the computation of DWT involves filtering, an efficient filtering process is essential in DWT hardware implementation. In the multistage DWT, coefficients are calculated recursively, and in addition to the wavelet decomposition stage, extra space is required to store the intermediate coefficients. Hence, the overall performance depends significantly on the precision of the intermediate DWT coefficients. This work presents new implementation techniques of DWT, that are efficient in terms of computation, storage, and with better signal-to-noise ratio in the reconstructed signal.

ğŸ”¹ [Scalable and Efficient MoE Training for Multitask Multilingual Models](https://arxiv.org/abs/2109.10465)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2109.10465)
- ğŸ‘¤ **Authors:** Young Jin Kim, A. A. Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, H. Awadalla
- ğŸ—“ï¸ **Date:** 2021-09-22
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** The Mixture of Experts (MoE) models are an emerging class of sparsely activated deep learning models that have sublinear compute costs with respect to their parameters. In contrast with dense models, the sparse architecture of MoE offers opportunities for drastically growing model size with significant accuracy gain while consuming much lower compute budget. However, supporting large scale MoE training also has its own set of system and modeling challenges. To overcome the challenges and embrace the opportunities of MoE, we first develop a system capable of scaling MoE models efficiently to trillions of parameters. It combines multi-dimensional parallelism and heterogeneous memory technologies harmoniously with MoE to empower 8x larger models on the same hardware compared with existing work. Besides boosting system efficiency, we also present new training methods to improve MoE sample efficiency and leverage expert pruning strategy to improve inference time efficiency. By combining the efficient system and training methods, we are able to significantly scale up large multitask multilingual models for language generation which results in a great improvement in model accuracy. A model trained with 10 billion parameters on 50 languages can achieve state-of-the-art performance in Machine Translation (MT) and multilingual natural language generation tasks. The system support of efficient MoE training has been implemented and open-sourced with the DeepSpeed library.

ğŸ”¹ [Inferring multi-scale neural mechanisms with brain network modelling](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** M. Schirner, A. Mcintosh, Viktor Jirsa, G. Deco, P. Ritter
- ğŸ—“ï¸ **Date:** 2017-06-28
- ğŸ“‘ **Publisher:** bioRxiv
- ğŸ“ **Abstract:** The neurophysiological processes underlying non-invasive brain activity measurements are not well understood. Here, we developed a novel connectome-based brain network model that integrates individual structural and functional data with neural population dynamics to support multi-scale neurophysiological inference. Simulated populations were linked by structural connectivity and, as a novelty, driven by electroencephalography (EEG) source activity. Simulations not only predicted subjectsâ€™ individual resting-state functional magnetic resonance imaging (fMRI) time series and spatial network topologies over 20 minutes of activity, but more importantly, they also revealed precise neurophysiological mechanisms that underlie and link six empirical observations from different scales and modalities: (1) slow resting-state fMRI oscillations, (2) spatial topologies of functional connectivity networks, (3) excitation-inhibition balance, (4, 5) pulsed inhibition on short and long time scales, and (6) fMRI power-law scaling. These findings underscore the potential of this new modelling framework for general inference and integration of neurophysiological knowledge to complement empirical studies.

ğŸ”¹ [Massively scalable Sinkhorn distances via the NystrÃ¶m method](https://arxiv.org/abs/1812.05189)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1812.05189)
- ğŸ‘¤ **Authors:** Jason M. Altschuler, F. Bach, Alessandro Rudi, J. Weed
- ğŸ—“ï¸ **Date:** 2018-12-12
- ğŸ“‘ **Publisher:** Neural Information Processing Systems
- ğŸ“ **Abstract:** The Sinkhorn "distance", a variant of the Wasserstein distance with entropic regularization, is an increasingly popular tool in machine learning and statistical inference. However, the time and memory requirements of standard algorithms for computing this distance grow quadratically with the size of the data, making them prohibitively expensive on massive data sets. In this work, we show that this challenge is surprisingly easy to circumvent: combining two simple techniques---the Nystrom method and Sinkhorn scaling---provably yields an accurate approximation of the Sinkhorn distance with significantly lower time and memory requirements than other approaches. We prove our results via new, explicit analyses of the Nystrom method and of the stability properties of Sinkhorn scaling. We validate our claims experimentally by showing that our approach easily computes Sinkhorn distances on data sets hundreds of times larger than can be handled by other techniques.

ğŸ”¹ [Spatio-Temporal Variational Gaussian Processes](https://arxiv.org/abs/2111.01732)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2111.01732)
- ğŸ‘¤ **Authors:** Oliver Hamelijnck, William J. Wilkinson, Niki A. Loppi, A. Solin, T. Damoulas
- ğŸ—“ï¸ **Date:** 2021-11-02
- ğŸ“‘ **Publisher:** Neural Information Processing Systems
- ğŸ“ **Abstract:** We introduce a scalable approach to Gaussian process inference that combines spatio-temporal filtering with natural gradient variational inference, resulting in a non-conjugate GP method for multivariate data that scales linearly with respect to time. Our natural gradient approach enables application of parallel filtering and smoothing, further reducing the temporal span complexity to be logarithmic in the number of time steps. We derive a sparse approximation that constructs a state-space model over a reduced set of spatial inducing points, and show that for separable Markov kernels the full and sparse cases exactly recover the standard variational GP, whilst exhibiting favourable computational properties. To further improve the spatial scaling we propose a mean-field assumption of independence between spatial locations which, when coupled with sparsity and parallelisation, leads to an efficient and accurate method for large spatio-temporal problems.

ğŸ”¹ [Multi-View Stereo by Temporal Nonparametric Fusion](https://arxiv.org/abs/1904.06397)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1904.06397)
- ğŸ‘¤ **Authors:** Yuxin Hou, Juho Kannala, A. Solin
- ğŸ—“ï¸ **Date:** 2019-04-12
- ğŸ“‘ **Publisher:** IEEE International Conference on Computer Vision
- ğŸ“ **Abstract:** We propose a novel idea for depth estimation from multi-view image-pose pairs, where the model has capability to leverage information from previous latent-space encodings of the scene. This model uses pairs of images and poses, which are passed through an encoder-decoder model for disparity estimation. The novelty lies in soft-constraining the bottleneck layer by a nonparametric Gaussian process prior. We propose a pose-kernel structure that encourages similar poses to have resembling latent spaces. The flexibility of the Gaussian process (GP) prior provides adapting memory for fusing information from nearby views. We train the encoder-decoder and the GP hyperparameters jointly end-to-end. In addition to a batch method, we derive a lightweight estimation scheme that circumvents standard pitfalls in scaling Gaussian process inference, and demonstrate how our scheme can run in real-time on smart devices.

ğŸ”¹ [Efficient hardware architecture of softmax layer in deep neural network](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Bo Yuan
- ğŸ—“ï¸ **Date:** 2016-09-01
- ğŸ“‘ **Publisher:** ACM Symposium on Cloud Computing
- ğŸ“ **Abstract:** Deep neural network (DNN) has emerged as a very important machine learning and pattern recognition technique in the big data era. Targeting to different types of training and inference tasks, the structure of DNN varies with flexible choices of different component layers, such as fully connection layer, convolutional layer, pooling layer and softmax layer. Deviated from other layers that only require simple operations like addition or multiplication, the softmax layer contains expensive exponentiation and division, thereby causing the hardware design of softmax layer suffering from high complexity, long critical path delay and overflow problems. This paper, for the first time, presents efficient hardware architecture of softmax layer in DNN. By utilizing the domain transformation technique and down-scaling approach, the proposed hardware architecture avoids the aforementioned problems. Analysis shows that the proposed hardware architecture achieves reduced hardware complexity and critical path delay.

ğŸ”¹ [Scaled Vecchia Approximation for Fast Computer-Model Emulation](https://arxiv.org/abs/2005.00386)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2005.00386)
- ğŸ‘¤ **Authors:** M. Katzfuss, J. Guinness, E. Lawrence
- ğŸ—“ï¸ **Date:** 2020-05-01
- ğŸ“‘ **Publisher:** SIAM/ASA J. Uncertain. Quantification
- ğŸ“ **Abstract:** Many scientific phenomena are studied using computer experiments consisting of multiple runs of a computer model while varying the input settings. Gaussian processes (GPs) are a popular tool for the analysis of computer experiments, enabling interpolation between input settings, but direct GP inference is computationally infeasible for large datasets. We adapt and extend a powerful class of GP methods from spatial statistics to enable the scalable analysis and emulation of large computer experiments. Specifically, we apply Vecchia's ordered conditional approximation in a transformed input space, with each input scaled according to how strongly it relates to the computer-model response. The scaling is learned from the data, by estimating parameters in the GP covariance function using Fisher scoring. Our methods are highly scalable, enabling estimation, joint prediction and simulation in near-linear time in the number of model runs. In several numerical examples, our approach substantially outperformed existing methods.

ğŸ”¹ [The Phylogenetic Likelihood Library](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** T. Flouri, F. Izquierdo-Carrasco, D. Darriba, A. Aberer, Lam-Tung Nguyen, B. Minh, A. von Haeseler, A. Stamatakis
- ğŸ—“ï¸ **Date:** 2014-10-30
- ğŸ“‘ **Publisher:** Systematic Biology
- ğŸ“ **Abstract:** We introduce the Phylogenetic Likelihood Library (PLL), a highly optimized application programming interface for developing likelihood-based phylogenetic inference and postanalysis software. The PLL implements appropriate data structures and functions that allow users to quickly implement common, error-prone, and labor-intensive tasks, such as likelihood calculations, model parameter as well as branch length optimization, and tree space exploration. The highly optimized and parallelized implementation of the phylogenetic likelihood function and a thorough documentation provide a framework for rapid development of scalable parallel phylogenetic software. By example of two likelihood-based phylogenetic codes we show that the PLL improves the sequential performance of current software by a factor of 2â€“10 while requiring only 1 month of programming time for integration. We show that, when numerical scaling for preventing floating point underflow is enabled, the double precision likelihood calculations in the PLL are up to 1.9 times faster than those in BEAGLE. On an empirical DNA dataset with 2000 taxa the AVX version of PLL is 4 times faster than BEAGLE (scaling enabled and required). The PLL is available at http://www.libpll.org under the GNU General Public License (GPL).

ğŸ”¹ [Efficient Correlated Topic Modeling with Topic Embedding](https://arxiv.org/abs/1707.00206)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1707.00206)
- ğŸ‘¤ **Authors:** Junxian He, Zhiting Hu, Taylor Berg-Kirkpatrick, Ying Huang, E. Xing
- ğŸ—“ï¸ **Date:** 2017-07-01
- ğŸ“‘ **Publisher:** Knowledge Discovery and Data Mining
- ğŸ“ **Abstract:** Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.

ğŸ”¹ [An empirical analysis of dropout in piecewise linear networks](https://arxiv.org/abs/1312.6197)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/1312.6197)
- ğŸ‘¤ **Authors:** David Warde-Farley, I. Goodfellow, Aaron C. Courville, Yoshua Bengio
- ğŸ—“ï¸ **Date:** 2013-12-21
- ğŸ“‘ **Publisher:** International Conference on Learning Representations
- ğŸ“ **Abstract:** The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.

ğŸ”¹ [On measuring selection in experimental evolution](https://arxiv.org/abs/None)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/None)
- ğŸ‘¤ **Authors:** Luisâ€Miguel Chevin
- ğŸ—“ï¸ **Date:** 2010-09-01
- ğŸ“‘ **Publisher:** Biology Letters
- ğŸ“ **Abstract:** Distributions of mutation fitness effects from evolution experiments are available in an increasing number of species, opening the way for a vast array of applications in evolutionary biology. However, comparison of estimated distributions among studies is hampered by inconsistencies in the definitions of fitness effects and selection coefficients. In particular, the use of ratios of Malthusian growth rates as â€˜relative fitnessesâ€™ leads to wrong inference of the strength of selection. Scaling Malthusian fitness by the generation time may help overcome this shortcoming, and allow accurate comparison of selection coefficients across species. For species reproducing by binary fission (neglecting cellular death), ln2 can be used as a correction factor, but in general, the growth rate and generation time of the wild-type should be provided in studies reporting distribution of mutation fitness effects. I also discuss how density and frequency dependence of population growth affect selection and its measurement in evolution experiments.



Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters

Tree Search for Language Model Agents

Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models

CodeMonkeys: Scaling Test-Time Compute for Software Engineering

SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer

O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning


